# Awesome-CoT-Finetuning

A collection of paper and code for the chain of thought finetuning (CoT-Finetuning). We are looking forward to other participants to share their papers and codes. If interested, please contact chenxs@nudt.edu.cn or xschenranker@gmail.com. :fire: :fire: :fire: 

- We reproduced the code of all collected papers and tried to compare them under the same training framework. The reproduced code can be viewed in the [code](./code). :fire: :fire: :fire:
- We will release all datasets we construct in the same way as described in all collected papers in the future.

:bell: :bell: :bell: Update at July 2024


# Bookmarks
- [Survey Papers](#survey-papers-)
- [Datasets](#datasets-)
- [Chain-of-Thought Distillation](#Chain-of-Thought-Distillation-)
- [Self-Enhancement](#Self-Enhancement-)
- [Application](#Applicationt-)

## Survey Papers <span id="survey-papers-"></span>
| **Year**   | **Title**                                                                                     |  **Venue**    |                                       **Paper**                                            | **Code** |
| ---- |----------------------------------------------------------------------------------|:--------:|:---------------------------------------------------------------------------------:|:----:|
| 2024  | **Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future**   |  ACL    |                   [Link](https://arxiv.org/abs/2309.15402)                    | -  |

## Datasets <span id="datasets-"></span>
| **Type**                                                                                     |  **Dataset**    |                                       **Description**                                            | **Download** |
|----------------------------------------------------------------------------------|:--------:|:---------------------------------------------------------------------------------:|:----:|
| QA-Commonsense   |  CommonsenseQA    |                   [Link](https://arxiv.org/abs/1811.00937)                    | [Link](https://www.tau-nlp.org/commonsenseqa)  |

## Chain-of-Thought Distillation <span id="Chain-of-Thought-Distillation-"></span>
| **Year**   | **Title**                                                                                     |  **Venue**    |                                       **Paper**                                            | **Code** |
| ---- |----------------------------------------------------------------------------------|:--------:|:---------------------------------------------------------------------------------:|:----:|
| 2024  | **Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data**   |  AAAI    |                   [Link](https://ojs.aaai.org/index.php/AAAI/article/view/29821)                    | [Link](https://github.com/Yiwei98/TDG)   |

## Self-Enhancement <span id="Self-Enhancement-"></span>
| **Year**   | **Title**                                                                                     |  **Venue**    |                                       **Paper**                                            | **Code** |
| ---- |----------------------------------------------------------------------------------|:--------:|:---------------------------------------------------------------------------------:|:----:|
| 2022  | **Large Language Models Can Self-Improve**   |  ACL    |                   [Link](https://aclanthology.org/2023.emnlp-main.67/)                    | -  |

## Application <span id="Applicationt-"></span>
| **Year**   | **Title**                                                                                     |  **Venue**    |                                       **Paper**                                            | **Code** |
| ---- |----------------------------------------------------------------------------------|:--------:|:---------------------------------------------------------------------------------:|:----:|
| 2024  | **Effective Distillation of Table-based Reasoning Ability from LLMs**   |  ACL    |                   [Link](https://aclanthology.org/2024.lrec-main.492/)                    | [Link]([https://github.com/Yiwei98/TDG](https://github.com/Bernard-Yang/DistillTableCoT))  |
